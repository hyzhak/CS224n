{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CS224N: NLP with Deep Learning | Winter 2019 | Lecture 6 – Language Models and RNNs\n",
    "- [Lecture 6](https://www.youtube.com/watch?v=iWea12EAu6U) – Language Models and RNNs\n",
    "- [Lecture 7](https://www.youtube.com/watch?v=QEw0qEa0E50) – Vanishing Gradients, Fancy RNNs\n",
    "- [details](http://web.stanford.edu/class/cs224n/index.html#schedule) - assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model\n",
    "\n",
    "We use Language model to get probobility of next word based on previous words\n",
    "\n",
    "### Motivation\n",
    "- benchmark task - that helps us measure our progress on understanding language\n",
    "- subcomponent of tasks generating text, estimating probability of text and etc.\n",
    "\n",
    "\n",
    "### Applications\n",
    "- sensenses classification (who is speaking)\n",
    "- encoding module (e.g QA system)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gram\n",
    "#### trade-off between size of n-gram and sparsety\n",
    "- sometimes last n words isn't enough to know what should be next - **solution:** increase n\n",
    "- but when we get bigger $n$ we get bigger data sparsity\n",
    "- but because word `w` never occure in context of words' sequence $w_1,...,w_i$ there will be `0` chance to get that word -> **solution:** add small delta to all words - smooth probobility\n",
    "- but if context of works $w_1,...,w_i$ never was in data set we would have devision by `0` -> **solution:** backoff to smaller n, $w_2,...,w_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a fixed-window neural Language Model\n",
    "### Steps\n",
    "1. get words (in window)\n",
    "2. one-hot vector for each word (x)\n",
    "3. reflect to embeding ($e_i$)\n",
    "4. concatenate together ($e = [e_1,...e_i]$)\n",
    "5. hidden layer: $h = f(We + b_1)$\n",
    "6. output distribution: $\\hat{y} = softmax(Uh + b_2) \\in R^{|V|}$\n",
    "\n",
    "### Improvements\n",
    "- no sparsity problem\n",
    "- don't need to store all observer n-grams\n",
    "\n",
    "### Remaining problems\n",
    "- window never can be large enough\n",
    "- enlarging window enlarges $W$\n",
    "- $x^{(1)}$ and $x^{(2)}$ are multiplied by completely different weights in W. No symmetry in how the inputs are processed. - so what you learn in one section of metrix doesn't share with other sections, what leads to inefficient learning of word embedding in W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN\n",
    "1. get words (in window)\n",
    "2. one-hot vector for each word (x)\n",
    "3. reflect to embeding ($e_i$)\n",
    "4. \n",
    "$$\n",
    "h^{(t)} = \\sigma{(W_{h}h^{(t-1)} + W_{e}e^{t} + b_1)}\n",
    "$$\n",
    "$h^{(0)}$ - the initial hidden state\n",
    "5. output $\\hat{y} = softmax(Uh^{(t)} + b_2) \\in R^{|V|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps\n",
    "- big corpus of texts $x^{(1)},...,x^{(T)}$\n",
    "- feed into RNN-LM (Language Model); compute output distribution $\\hat{t^{(t)}}$ for every step $t$, predict probability dist of every word, given words so far\n",
    "- loss function on step $t$ is cross-entropy between predicted probability distribution $\\hat{y}^{(t)}$, and the true next word $y^{(t)}$ (one-hot for $x^{(t+1)}$):\n",
    "$$\n",
    "J^{(t)}(\\theta) = CE(y^{(t)}, \\hat{y}^{(t)}) = - \\sum_{w \\in V} y_w^{(t)} \\log \\hat{y}^{(t)}_w = - \\log \\hat{y}^{(t)}_{t+1}\n",
    "$$\n",
    "- average this to get overall loss for entire training set:\n",
    "$$\n",
    "J(\\theta) = \\frac{1}{T} \\sum_{t=1}^{T} J^{(t)}(\\theta) = - \\frac{1}{T} \\sum_{t=1}^{T} \\log \\hat{y}^{(t)}_{t+1}\n",
    "$$\n",
    "- coputing loss for entire corpus is too expensive, so we use sentense or document from corpus (Stochastic Gradient Descent)\n",
    "- derivative of $J(\\theta)$ w.r.t. (with respect to) the repeated weight matrix $W_h$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{J^{(t)}}}{\\partial{W_h}} = \\sum_{i=1}^{t}\\frac{\\partial{J^{(t)}}}{\\partial{W_h}}|_{i}\n",
    "$$\n",
    "backpropagate over timesteps $i=t,...,0$, summing gradients as you go. This algorithm is called \"backpropagation through time\"\n",
    "\n",
    "### pros\n",
    "- can process **any length input**\n",
    "- computation for step `t` can (in theory) use information from **many steps back**\n",
    "- **model size doesn't increase** for longer input\n",
    "- same weights apply one every timestep, so there is symmetry in how inputs are processed\n",
    "\n",
    "### cons\n",
    "- can't process sequence in parallel (slow)\n",
    "- in practice, it's difficult to access information many steps back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Language Model\n",
    "the lower is better\n",
    "$$\n",
    "perplexity = \\prod_{t=1}^{T}(\\frac{1}{P_{LM}(x^{(t+1)} | x^{(t)},...,x^{(1)}})^{1/T}\n",
    "$$\n",
    "\n",
    "- inverse probability of corpus, according to Language Model\n",
    "- $1/T$ - normalized by number of words - need it because with bigger corpus it would be smaller and smaller\n",
    "\n",
    "is equal to the exponential of the cross-entropy loss $J(\\theta)$:\n",
    "$$\n",
    "= \\prod_{t=1}^{T}(\\frac{1}{\\hat{y}^{(t)}_{x_{t+1}}}) = \\exp (1/T \\sum_{t=1}^{T} - \\log \\hat{y}^{(t)}_{x_{t+1}} ) = exp(J(\\theta))\n",
    "$$\n",
    "\n",
    "comparision different model in Facebook: [Building an Efficient Neural Language Model Over a Billion Words](https://research.fb.com/building-an-efficient-neural-language-model-over-a-billion-words/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture06-rnnlm.pdf\n",
    "\n",
    "https://youtu.be/iWea12EAu6U?t=3251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
